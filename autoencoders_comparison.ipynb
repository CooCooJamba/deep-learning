{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Autoencoders Comparison: Fully Connected Autoencoder vs Variational Autoencoder\n",
    "\n",
    "This script implements and compares two types of autoencoders:\n",
    "1. Fully Connected Autoencoder for MNIST with Gaussian noise\n",
    "2. Convolutional Variational Autoencoder (VAE) for face image generation\n",
    "\n",
    "Author: VShulgin\n",
    "Date: 2022-08-20\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple, List, Optional\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"Add Gaussian noise to tensor for denoising autoencoder\"\"\"\n",
    "    def __init__(self, mean: float = 0.0, std: float = 0.1):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "def prepare_mnist_data(batch_size: int = 100, noise_std: float = 0.1) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Prepare MNIST dataset with Gaussian noise for denoising autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for data loader\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader for MNIST dataset\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        AddGaussianNoise(std=noise_std)\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class FullyConnectedAutoencoder(nn.Module):\n",
    "    \"\"\"Fully Connected Autoencoder for MNIST dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 784, latent_dim: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the autoencoder architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input data\n",
    "            latent_dim: Dimension of latent space\n",
    "        \"\"\"\n",
    "        super(FullyConnectedAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through autoencoder\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "def train_autoencoder(model: nn.Module, \n",
    "                     data_loader: DataLoader, \n",
    "                     epochs: int = 10, \n",
    "                     lr: float = 0.001) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train an autoencoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: Autoencoder model to train\n",
    "        data_loader: DataLoader for training data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        List of training losses per epoch\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    print(\"Training Autoencoder...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (data, _) in enumerate(data_loader):\n",
    "            # Flatten images\n",
    "            img = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1:2d}/{epochs}] | Loss: {avg_loss:.6f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def visualize_reconstructions(model: nn.Module, \n",
    "                            data_loader: DataLoader, \n",
    "                            num_samples: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Visualize original and reconstructed images.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained autoencoder model\n",
    "        data_loader: DataLoader for test data\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataiter = iter(data_loader)\n",
    "        img, _ = next(dataiter)\n",
    "        img_flat = img.view(img.size(0), -1).to(device)\n",
    "        output = model(img_flat)\n",
    "        \n",
    "        # Reshape back to images\n",
    "        img = img.cpu().numpy()\n",
    "        output = output.view(output.size(0), 1, 28, 28).cpu().numpy()\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(2, num_samples, figsize=(15, 4))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Original images (top row)\n",
    "            axes[0, i].imshow(img[i].squeeze(), cmap='gray')\n",
    "            axes[0, i].set_title('Original')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstructed images (bottom row)\n",
    "            axes[1, i].imshow(output[i].squeeze(), cmap='gray')\n",
    "            axes[1, i].set_title('Reconstructed')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Autoencoder Reconstructions', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('autoencoder_reconstructions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"Convolutional Variational Autoencoder for image generation\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int = 3, \n",
    "                 base_channels: int = 32, \n",
    "                 latent_dim: int = 1024):\n",
    "        \"\"\"\n",
    "        Initialize the VAE architecture.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            base_channels: Base number of channels for convolutional layers\n",
    "            latent_dim: Dimension of latent space\n",
    "        \"\"\"\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: (3, 64, 64) -> (32, 32, 32)\n",
    "            nn.Conv2d(in_channels, base_channels, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (32, 32, 32) -> (64, 16, 16)\n",
    "            nn.Conv2d(base_channels, base_channels * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (64, 16, 16) -> (128, 8, 8)\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (128, 8, 8) -> (256, 4, 4)\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Latent space layers\n",
    "        self.fc_mu = nn.Linear(base_channels * 8 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(base_channels * 8 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Decoder input\n",
    "        self.decoder_input = nn.Linear(latent_dim, base_channels * 8 * 4 * 4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (256, 4, 4) -> (128, 8, 8)\n",
    "            nn.ConvTranspose2d(base_channels * 8, base_channels * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (128, 8, 8) -> (64, 16, 16)\n",
    "            nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (64, 16, 16) -> (32, 32, 32)\n",
    "            nn.ConvTranspose2d(base_channels * 2, base_channels, 4, 2, 1),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # (32, 32, 32) -> (3, 64, 64)\n",
    "            nn.ConvTranspose2d(base_channels, in_channels, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode input into latent distribution parameters\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reparameterization trick for sampling\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode latent vector into reconstructed image\"\"\"\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 256, 4, 4)  # Reshape to match encoder output\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Full forward pass through VAE\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x: torch.Tensor, \n",
    "            x: torch.Tensor, \n",
    "            mu: torch.Tensor, \n",
    "            logvar: torch.Tensor, \n",
    "            reconstruction_weight: float = 1.0,\n",
    "            kl_weight: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    VAE loss function combining reconstruction loss and KL divergence.\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed images\n",
    "        x: Original images\n",
    "        mu: Mean of latent distribution\n",
    "        logvar: Log variance of latent distribution\n",
    "        reconstruction_weight: Weight for reconstruction loss\n",
    "        kl_weight: Weight for KL divergence\n",
    "        \n",
    "    Returns:\n",
    "        Total loss\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return reconstruction_weight * recon_loss + kl_weight * kl_loss\n",
    "\n",
    "def prepare_face_data(data_path: str, batch_size: int = 32) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Prepare face image dataset for VAE training.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to face images\n",
    "        batch_size: Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader for face dataset\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.ImageFolder(data_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train_vae(model: nn.Module, \n",
    "             data_loader: DataLoader, \n",
    "             epochs: int = 20, \n",
    "             lr: float = 0.001) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train a Variational Autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        model: VAE model to train\n",
    "        data_loader: DataLoader for training data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        List of training losses per epoch\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    print(\"Training Variational Autoencoder...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (data, _) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1:2d}/{epochs}] | Loss: {avg_loss:.6f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_samples(model: nn.Module, \n",
    "                    num_samples: int = 25, \n",
    "                    latent_dim: int = 1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate new samples from trained VAE.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        num_samples: Number of samples to generate\n",
    "        latent_dim: Dimension of latent space\n",
    "        \n",
    "    Returns:\n",
    "        Generated images\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        samples = model.decode(z)\n",
    "    return samples.cpu()\n",
    "\n",
    "def visualize_generated_samples(samples: torch.Tensor, \n",
    "                               grid_size: Tuple[int, int] = (5, 5)) -> None:\n",
    "    \"\"\"\n",
    "    Visualize generated samples from VAE.\n",
    "    \n",
    "    Args:\n",
    "        samples: Generated images tensor\n",
    "        grid_size: Grid size for visualization\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(samples):\n",
    "            img = samples[i].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('VAE Generated Samples', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vae_generated_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run both autoencoder experiments\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"AUTOENCODERS COMPARISON EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Part 1: Fully Connected Autoencoder for MNIST\n",
    "    print(\"\\n1. Training Fully Connected Autoencoder on MNIST\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    mnist_loader = prepare_mnist_data(batch_size=100, noise_std=0.1)\n",
    "    fc_autoencoder = FullyConnectedAutoencoder(latent_dim=3)\n",
    "    \n",
    "    # Train autoencoder\n",
    "    fc_losses = train_autoencoder(fc_autoencoder, mnist_loader, epochs=10, lr=0.001)\n",
    "    \n",
    "    # Visualize reconstructions\n",
    "    visualize_reconstructions(fc_autoencoder, mnist_loader, num_samples=10)\n",
    "    \n",
    "    # Part 2: Variational Autoencoder for Face Generation\n",
    "    print(\"\\n2. Training Variational Autoencoder on Face Images\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Extract face data if needed\n",
    "    face_data_path = \"faces_data\"\n",
    "    if not os.path.exists(face_data_path):\n",
    "        print(\"Please extract faces.zip to faces_data/ directory\")\n",
    "        return\n",
    "    \n",
    "    # Prepare face data\n",
    "    face_loader = prepare_face_data(face_data_path, batch_size=32)\n",
    "    \n",
    "    # Create and train VAE\n",
    "    vae = VariationalAutoencoder(latent_dim=1024)\n",
    "    vae_losses = train_vae(vae, face_loader, epochs=20, lr=0.001)\n",
    "    \n",
    "    # Generate and visualize samples\n",
    "    generated_samples = generate_samples(vae, num_samples=25)\n",
    "    visualize_generated_samples(generated_samples)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fc_losses, label='FC Autoencoder', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('FC Autoencoder Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(vae_losses, label='VAE', color='orange', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('VAE Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nExperiment completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
