{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CIFAR-10 LeNet Architecture Comparison with PyTorch\n",
    "\n",
    "This script compares different variations of LeNet architecture on the CIFAR-10 dataset\n",
    "using PyTorch. It evaluates original LeNet with Tanh, ReLU activation, MaxPooling,\n",
    "and a modernized version with additional convolutional layers.\n",
    "\n",
    "Author: VShulgin\n",
    "Date: 2022-08-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_cifar10_data(batch_size: int = 64) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Load and preprocess CIFAR-10 dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for data loaders\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Define transformations with normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    data_train = CIFAR10(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    data_test = CIFAR10(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        data_train, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        data_test, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def visualize_sample_images(data_loader: DataLoader, \n",
    "                          class_names: List[str], \n",
    "                          num_images: int = 9) -> None:\n",
    "    \"\"\"\n",
    "    Visualize sample images from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_loader: DataLoader containing images\n",
    "        class_names: List of class names\n",
    "        num_images: Number of images to display\n",
    "    \"\"\"\n",
    "    images, labels = next(iter(data_loader))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            # Convert tensor to numpy and denormalize\n",
    "            img = images[i].numpy().transpose(1, 2, 0)\n",
    "            img = np.clip(img * np.array([0.2023, 0.1994, 0.2010]) + \n",
    "                         np.array([0.4914, 0.4822, 0.4465]), 0, 1)\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f'Class: {class_names[labels[i].item()]}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_lenet_old() -> nn.Sequential:\n",
    "    \"\"\"Create original LeNet architecture with Tanh activation and AvgPool\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.Tanh(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.Tanh(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0),\n",
    "        nn.Tanh(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=120, out_features=84),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=84, out_features=10)\n",
    "    )\n",
    "\n",
    "def create_lenet_relu() -> nn.Sequential:\n",
    "    \"\"\"Create LeNet with ReLU activation instead of Tanh\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=120, out_features=84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=84, out_features=10)\n",
    "    )\n",
    "\n",
    "def create_lenet_maxpool() -> nn.Sequential:\n",
    "    \"\"\"Create LeNet with ReLU activation and MaxPooling\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=120, out_features=84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=84, out_features=10)\n",
    "    )\n",
    "\n",
    "def create_lenet_modern() -> nn.Sequential:\n",
    "    \"\"\"Create modernized LeNet with additional convolutional layers\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Conv2d(in_channels=6, out_channels=6, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=120, out_features=84),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=84, out_features=10)\n",
    "    )\n",
    "\n",
    "def train_model(model: nn.Module, \n",
    "               train_loader: DataLoader, \n",
    "               test_loader: DataLoader, \n",
    "               num_epochs: int = 10,\n",
    "               model_name: str = \"Model\") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: Training data loader\n",
    "        test_loader: Test data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        model_name: Name of the model for printing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses, test_accuracies)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                \n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}, '\n",
    "                  f'Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"{model_name} training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_losses, test_accuracies\n",
    "\n",
    "def plot_results(loss_dict: Dict[str, Tuple[List[float], List[float], List[float]]]) -> None:\n",
    "    \"\"\"\n",
    "    Plot training and test results for all models.\n",
    "    \n",
    "    Args:\n",
    "        loss_dict: Dictionary with model names as keys and \n",
    "                  (train_losses, test_losses, test_accuracies) as values\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    for model_name, (train_losses, test_losses, test_accuracies) in loss_dict.items():\n",
    "        axes[0].plot(train_losses, label=model_name, linewidth=2)\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Training Loss\")\n",
    "    axes[0].set_title(\"Training Loss Comparison\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test loss\n",
    "    for model_name, (train_losses, test_losses, test_accuracies) in loss_dict.items():\n",
    "        axes[1].plot(test_losses, label=model_name, linewidth=2)\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Test Loss\")\n",
    "    axes[1].set_title(\"Test Loss Comparison\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    for model_name, (train_losses, test_losses, test_accuracies) in loss_dict.items():\n",
    "        axes[2].plot(test_accuracies, label=model_name, linewidth=2)\n",
    "    axes[2].set_xlabel(\"Epoch\")\n",
    "    axes[2].set_ylabel(\"Test Accuracy (%)\")\n",
    "    axes[2].set_title(\"Test Accuracy Comparison\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lenet_architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the LeNet architecture comparison.\"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    train_loader, test_loader = load_cifar10_data(batch_size=64)\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    class_names = [\n",
    "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "    ]\n",
    "    \n",
    "    # Visualize sample images\n",
    "    print(\"Visualizing sample images...\")\n",
    "    visualize_sample_images(train_loader, class_names)\n",
    "    \n",
    "    # Create and train different LeNet variants\n",
    "    models = {\n",
    "        \"LeNet_Original\": create_lenet_old(),\n",
    "        \"LeNet_ReLU\": create_lenet_relu(),\n",
    "        \"LeNet_MaxPool\": create_lenet_maxpool(),\n",
    "        \"LeNet_Modern\": create_lenet_modern()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        train_losses, test_losses, test_accuracies = train_model(\n",
    "            model, train_loader, test_loader, \n",
    "            num_epochs=30, model_name=model_name\n",
    "        )\n",
    "        results[model_name] = (train_losses, test_losses, test_accuracies)\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"Plotting comparison results...\")\n",
    "    plot_results(results)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    for model_name, (train_losses, test_losses, test_accuracies) in results.items():\n",
    "        print(f\"{model_name:<20}: Test Loss = {test_losses[-1]:.4f}, \"\n",
    "              f\"Accuracy = {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
