{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FashionMNIST Optimizers Comparison with PyTorch\n",
    "\n",
    "This script compares different optimization algorithms on the FashionMNIST dataset\n",
    "using PyTorch. It evaluates SGD with momentum, AdaGrad, RMSProp, Adam, AdaDelta,\n",
    "and Adamax optimizers.\n",
    "\n",
    "Author: VShulgin\n",
    "Date: 2022-07-28\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_fashion_mnist_data() -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load and preprocess FashionMNIST dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (x_train, y_train, x_test, y_test) tensors\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Extract data and labels\n",
    "    x_train = train_dataset.data.float() / 255.0\n",
    "    y_train = train_dataset.targets\n",
    "    x_test = test_dataset.data.float() / 255.0\n",
    "    y_test = test_dataset.targets\n",
    "    \n",
    "    # Flatten images\n",
    "    x_train = x_train.view(-1, 28 * 28)\n",
    "    x_test = x_test.view(-1, 28 * 28)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def visualize_sample_images(x_data: torch.Tensor, y_data: torch.Tensor, \n",
    "                          class_names: List[str], num_images: int = 9) -> None:\n",
    "    \"\"\"\n",
    "    Visualize sample images from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        x_data: Image data tensor\n",
    "        y_data: Labels tensor\n",
    "        class_names: List of class names\n",
    "        num_images: Number of images to display\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < num_images:\n",
    "            img = x_data[i].view(28, 28).numpy()\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(f'Class: {class_names[y_data[i].item()]}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class FashionMNISTNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for FashionMNIST classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int = 784, hidden_size: int = 128, \n",
    "                 output_size: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the network architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input features\n",
    "            hidden_size: Size of hidden layer\n",
    "            output_size: Number of output classes\n",
    "        \"\"\"\n",
    "        super(FashionMNISTNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "def test_optimizer(net: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "                  x_train: torch.Tensor, y_train: torch.Tensor,\n",
    "                  x_test: torch.Tensor, y_test: torch.Tensor,\n",
    "                  batch_size: int = 100, epochs: int = 50) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Test an optimizer on the FashionMNIST dataset.\n",
    "    \n",
    "    Args:\n",
    "        net: Neural network model\n",
    "        optimizer: Optimizer to test\n",
    "        x_train: Training data\n",
    "        y_train: Training labels\n",
    "        x_test: Test data\n",
    "        y_test: Test labels\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses)\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_values_train = []\n",
    "    loss_values_test = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        net.train()\n",
    "        order = np.random.permutation(len(x_train))\n",
    "        \n",
    "        for start_index in range(0, len(x_train), batch_size):\n",
    "            batch_index = order[start_index:start_index + batch_size]\n",
    "            x_batch = x_train[batch_index]\n",
    "            y_batch = y_train[batch_index]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_preds = net(x_batch)\n",
    "            loss_val = criterion(y_preds, y_batch)\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            # Training loss\n",
    "            y_pred_train = net(x_train)\n",
    "            loss_train = criterion(y_pred_train, y_train)\n",
    "            loss_values_train.append(loss_train.item())\n",
    "            \n",
    "            # Test loss\n",
    "            y_pred_test = net(x_test)\n",
    "            loss_test = criterion(y_pred_test, y_test)\n",
    "            loss_values_test.append(loss_test.item())\n",
    "    \n",
    "    return loss_values_train, loss_values_test\n",
    "\n",
    "def plot_loss_curves(loss_dict: Dict[str, Tuple[List[float], List[float]]], \n",
    "                    title_suffix: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot training and test loss curves for different optimizers.\n",
    "    \n",
    "    Args:\n",
    "        loss_dict: Dictionary with optimizer names as keys and (train_losses, test_losses) as values\n",
    "        title_suffix: Suffix for plot titles\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for optim_name, (train_losses, test_losses) in loss_dict.items():\n",
    "        plt.plot(train_losses, label=optim_name, linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-Entropy Loss\")\n",
    "    plt.title(f\"Training Loss - {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for optim_name, (train_losses, test_losses) in loss_dict.items():\n",
    "        plt.plot(test_losses, label=optim_name, linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-Entropy Loss\")\n",
    "    plt.title(f\"Test Loss - {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the optimizer comparison.\"\"\"\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading FashionMNIST dataset...\")\n",
    "    x_train, y_train, x_test, y_test = load_fashion_mnist_data()\n",
    "    \n",
    "    print(f\"Training data shape: {x_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {x_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "    \n",
    "    # Class names for FashionMNIST\n",
    "    class_names = [\n",
    "        'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]\n",
    "    \n",
    "    # Visualize sample images\n",
    "    print(\"Visualizing sample images...\")\n",
    "    visualize_sample_images(x_train.view(-1, 28, 28), y_train, class_names)\n",
    "    \n",
    "    # Test different optimizers\n",
    "    loss_optim = {}\n",
    "    optimizers_config = {\n",
    "        \"SGD with Momentum\": {\n",
    "            \"optimizer\": lambda params: torch.optim.SGD(params, lr=0.01, momentum=0.9),\n",
    "            \"hidden_size\": 128\n",
    "        },\n",
    "        \"AdaGrad\": {\n",
    "            \"optimizer\": lambda params: torch.optim.Adagrad(params, lr=0.01),\n",
    "            \"hidden_size\": 128\n",
    "        },\n",
    "        \"RMSProp\": {\n",
    "            \"optimizer\": lambda params: torch.optim.RMSprop(params, lr=0.001, weight_decay=1e-5),\n",
    "            \"hidden_size\": 128\n",
    "        },\n",
    "        \"Adam\": {\n",
    "            \"optimizer\": lambda params: torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999)),\n",
    "            \"hidden_size\": 128\n",
    "        },\n",
    "        \"AdaDelta\": {\n",
    "            \"optimizer\": lambda params: torch.optim.Adadelta(params, lr=1.0, rho=0.9),\n",
    "            \"hidden_size\": 128\n",
    "        },\n",
    "        \"Adamax\": {\n",
    "            \"optimizer\": lambda params: torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999)),\n",
    "            \"hidden_size\": 128\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Testing different optimizers...\")\n",
    "    for optim_name, config in optimizers_config.items():\n",
    "        print(f\"Testing {optim_name}...\")\n",
    "        net = FashionMNISTNet(\n",
    "            input_size=784, \n",
    "            hidden_size=config[\"hidden_size\"], \n",
    "            output_size=10\n",
    "        )\n",
    "        \n",
    "        optimizer = config[\"optimizer\"](net.parameters())\n",
    "        train_losses, test_losses = test_optimizer(\n",
    "            net, optimizer, x_train, y_train, x_test, y_test,\n",
    "            batch_size=100, epochs=50\n",
    "        )\n",
    "        \n",
    "        loss_optim[optim_name] = (train_losses, test_losses)\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"Plotting results...\")\n",
    "    plot_loss_curves(loss_optim, \"FashionMNIST Optimizers Comparison\")\n",
    "    \n",
    "    # Print final losses\n",
    "    print(\"\\nFinal Test Losses:\")\n",
    "    for optim_name, (train_losses, test_losses) in loss_optim.items():\n",
    "        print(f\"{optim_name}: {test_losses[-1]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
